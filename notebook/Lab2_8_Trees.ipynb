{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.8: Tree Based Methods\n",
    "\n",
    "In this case, we will be conduct a simpler exercise with decision trees using previous implementations. In particular, we will make use of several implemented methods in ML libraries s.a. `sklearn` (_that should be good news for you, doesn't it?_). With this, we will try to explore the main characteristics of decision trees, that you will also have to explore in the more theoretical part of the lab (the other exercise, the one on the pdf).\n",
    "\n",
    "We will begin, as usual, importing the relevant libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydotplus'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image  \n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error, confusion_matrix\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydotplus\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pydotplus'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mode\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# ML libraries to construct, use and analyse the trees\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz, DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from io import StringIO\n",
    "from IPython.display import Image  \n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix\n",
    "import pydotplus\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first apply this to a regression dataset so that you see the way the model is constructed for this case "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Tree\n",
    "\n",
    "For starters, let us try out a regression tree. To that end, first load the `Hitters.csv` dataset from the `data` \n",
    "folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Player', 'AtBat', 'Hits', 'HmRun', 'Runs', 'RBI', 'Walks', 'Years',\n",
      "       'CAtBat', 'CHits', 'CHmRun', 'CRuns', 'CRBI', 'CWalks', 'League',\n",
      "       'Division', 'PutOuts', 'Assists', 'Errors', 'Salary', 'NewLeague'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#(make sure you remove the None values!)\n",
    "hitters  = pd.read_csv('../data/Hitters.csv', sep=',')\n",
    "hitters = hitters.dropna()\n",
    "\n",
    "# Print the columns here to check their names\n",
    "print(hitters.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this first case, we will only use the variables `Years` and `Hits` for the tree. Our target variable will be `Salary`. Separate them into `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the independent variables (X) from the dependent one (y - salary)\n",
    " \n",
    "X = hitters[['Years', 'Hits']]\n",
    "y = hitters['Salary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a _decision tree regressor_ using the `sklearn` function and fit it. To do that, check out the `DecisionTreeRegressor` in sklearn and implement it here.\n",
    "\n",
    "For reproducibility, fix the `random_state` to `0` and the `max_leaf_nodes` to `3` (make sure you know what this last thing does!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(max_leaf_nodes=3, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(max_leaf_nodes=3, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(max_leaf_nodes=3, random_state=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct the regressor\n",
    "regressor = regressor = DecisionTreeRegressor(random_state=0, max_leaf_nodes=3)\n",
    "\n",
    "# Fit it with the .fit method\n",
    "regressor.fit(X, y)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will employ some functions engrained in `StringIO` alongside the method `export_graphviz` from the `tree` object in sklearn. This will enable us to visualize the constructed tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pydotplus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m dot_data \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[0;32m      2\u001b[0m export_graphviz(regressor, \n\u001b[0;32m      3\u001b[0m                 out_file\u001b[38;5;241m=\u001b[39mdot_data, \n\u001b[0;32m      4\u001b[0m                 feature_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYears\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHits\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m      5\u001b[0m                 filled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[0;32m      6\u001b[0m                 class_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m----> 8\u001b[0m graph \u001b[38;5;241m=\u001b[39m pydotplus\u001b[38;5;241m.\u001b[39mgraph_from_dot_data(dot_data\u001b[38;5;241m.\u001b[39mgetvalue())  \n\u001b[0;32m      9\u001b[0m Image(graph\u001b[38;5;241m.\u001b[39mcreate_png())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pydotplus' is not defined"
     ]
    }
   ],
   "source": [
    "dot_data = StringIO()\n",
    "export_graphviz(regressor, \n",
    "                out_file=dot_data, \n",
    "                feature_names=['Years', 'Hits'], \n",
    "                filled=True, \n",
    "                class_names=None)\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Question: Describe the previous tree. What do you see? Do you think this will work well? <br><br>\n",
    "Nuestro arbol cuenta tres nodos hojas obtenidos tras las condiciones de Years <= 4.5 y Hits <=117.5. Al tratarse de un arbol muy simple, no resultará útil para modelos complejos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will plot the decision regions using the information on the cuts. Add lines wherever needed so that you can see the decision boundaries for the regression tree above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitters.plot('Years', 'Hits', kind='scatter', color='orange', figsize=(7,6))\n",
    "plt.xlim(0,25)\n",
    "plt.ylim(ymin=-5)\n",
    "\n",
    "# Add whatever you may need here to clearly plot the decision boundaries \n",
    "plt.axvline(x=4.5, color='blue', linestyle='--', label='Years = 4.5')\n",
    "plt.axhline(y=117.5, xmin=4.5/25, xmax=1, color='red', linestyle='--', label='Hits = 117.5')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Size\n",
    "\n",
    "Now, for the previous part we limited the growth of the tree so that we recovered a simple (but easily interpretable) tree. Now we will go all-out: we will construct a more exhaustive tree using different variables. For this particular case, let us use **all variables except** `League`, `Division`, `NewLeague` and `Salary` as independent variables to predict, precisely, the `Salary` value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the input variable dataset\n",
    "X = hitters[['AtBat', 'Hits', 'HmRun', 'Runs', 'RBI', 'Walks', 'Years',\n",
    "       'CAtBat', 'CHits', 'CHmRun', 'CRuns', 'CRBI', 'CWalks',\n",
    "        'PutOuts', 'Assists', 'Errors']]\n",
    "\n",
    "# Print the column names to check\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now perform the train/test split, but we will do it so that the proportion of train and test examples is $50\\%$ (that is, train and test consist on $50\\%$ of the datapoints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the train-test split here. Do it so that the  \n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.5, train_size=0.5, random_state=0) #Fill the NAs, fixing also the random_state to 0 for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, train the tree to its fullest extent: put no limits on the growth and see what happens. You can re-use some of the previous `graphviz` code to visualize the tree here. Plot the complete tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the tree without limits to its growth (random_state = 0)\n",
    "unlimited_tree_regressor =DecisionTreeRegressor(random_state=0)\n",
    "unlimited_tree_regressor.fit(x_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuse the code you need to plot the tree here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dot_data = StringIO()\n",
    "export_graphviz(unlimited_tree_regressor, \n",
    "                out_file=dot_data, \n",
    "                feature_names=X.columns, \n",
    "                filled=True, \n",
    "                class_names=None)\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> Question: What do you see? What can you say about this tree? Does it have any important properties? <br><br>\n",
    "El árbol ha crecido completamente, alcanzando un punto en el que todos sus nodos terminales son completamente puros (coste = 0). Esto sugiere un posible sobreajuste a los datos, ya que las divisiones se han realizado hasta clasificar perfectamente cada caso. Como resultado, es probable que el modelo no tenga un buen desempeño al aplicarse a nuevos datos, ya que su capacidad de generalización se ve afectada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we went _a bit too far_ with the tree... Let's set up some limitations to see everything better. Try setting the `max_features` to 9, and the `max_depth` to 4. (_It is important you understand what these parameters do! Check out the documentation in the [library](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_2 =  DecisionTreeRegressor(max_features=9, random_state=0, max_depth=4)\n",
    "regressor_2.fit(x_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuse the code you may need to plot the tree here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "export_graphviz(regressor_2, \n",
    "                out_file=dot_data, \n",
    "                feature_names= x_train.columns, \n",
    "                filled=True, \n",
    "                class_names=None)\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see if this tree works well at all... Since we are performing regression, we can obtain the RMSE (we use the Root MSE since it shares the same dimensions of the outputs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = regressor_2.predict(x_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "# Print the RMSE for the predictions\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the question is: how do we know which tree depth to select here? As you may expect, the answer is, as almost always here, performing _cross validation_. In this particular instance we will not conduct exhaustive cross validation. Instead, we will do it in a very simple manner, obtaining *a single tree* for each depth value we want, fitting it to the data and seeing how well does it perform both in train and test  fitted to the data. To do this, do the following:\n",
    "* Fit a **fixed max depth** (`i`) decision tree regressor using *all `x_train` variables*. Also, *fix the `random state` to 1* for reproducibility.\n",
    "* Register its train and test RMSEs\n",
    "* Plot the train and test RMSE curves for each `i` depth  \n",
    "\n",
    "Make sure that you explore _enough_ depth values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store the results\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "\n",
    "# Range of depths to be explored\n",
    "tree_size = np.arange(1,20)\n",
    "\n",
    "for i in tree_size:\n",
    "\n",
    "    # Train the needed tree with the set depth, then measure its RMSE in train and test and store them in the previous lists\n",
    "    tree_regressor = DecisionTreeRegressor(max_depth=i, random_state=1)\n",
    "    tree_regressor.fit(x_train, y_train)\n",
    "\n",
    "    y_train_predict = tree_regressor.predict(x_train)\n",
    "    y_test_predict = tree_regressor.predict(x_test)\n",
    "\n",
    "    train_rmse_value = np.sqrt(mean_squared_error(y_train, y_train_predict))\n",
    "    test_rmse_value = np.sqrt(mean_squared_error(y_test, y_test_predict))\n",
    "\n",
    "    train_rmse.append(train_rmse_value)\n",
    "    test_rmse.append(test_rmse_value)\n",
    "\n",
    "# Plot the results\n",
    "plt.plot(tree_size, train_rmse, 'r*--', label=\"Train RMSE\")\n",
    "plt.plot(tree_size, test_rmse, 'b.-', label=\"Test RMSE\")\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Root Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.title('RMSE vs Tree Depth')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Question: What do you see here? What depth value would you select? <br><br>\n",
    "A medida que aumentamos la profundidad del árbol, el error en el conjunto de entrenamiento disminuye notablemente, lo que indica una alta varianza. Sin embargo, el error en el conjunto de prueba no muestra una reducción significativa, lo que sugiere un posible sobreajuste si max_depth es demasiado alto. Según la gráfica, el menor error en el test se encuentra aproximadamente en max_depth = 3, por lo que este valor sería el más adecuado, ya que logra un equilibrio entre el ajuste a los datos de entrenamiento y la capacidad de generalización.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are not really making CV, we do not have multiple values for the train and test RMSE for each tree. Therefore, we *do not* have errorbars in the previous plot. That should raise some suspicions from your part. \n",
    "\n",
    "> Question: What happens if we change the `random_state` value? Are the previous results robust? <br><br> Como se observa en el gráfico, el RMSE del conjunto de prueba varía significativamente según el valor de random_state, lo que indica que el modelo es sensible a la aleatoriedad. A partir de max_depth > 10, los valores de RMSE se estabilizan, lo que sugiere que el árbol es más influenciado por cómo se dividen los datos en las primeras etapas de decisión. Para obtener un modelo más robusto, la profundidad óptima debería minimizar el RMSE en promedio en varias ejecuciones con distintas semillas. En este caso, ese valor óptimo parece estar alrededor de 3.\n",
    "\n",
    "\n",
    "To answer the previous question you can try out code in the next cell. Feel free to try whatever you think is needed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store the results for different random states\n",
    "random_states = [0, 1, 42, 100] \n",
    "train_rmse_results = []\n",
    "test_rmse_results = []\n",
    "\n",
    "# Range of depths to be explored\n",
    "tree_size = np.arange(1, 20)\n",
    "\n",
    "for rs in random_states:\n",
    "    train_rmse = []\n",
    "    test_rmse = []\n",
    "\n",
    "    for i in tree_size:\n",
    "        tree_regressor = DecisionTreeRegressor(max_depth=i, random_state=rs)\n",
    "        tree_regressor.fit(x_train, y_train)\n",
    "\n",
    "        y_train_predict = tree_regressor.predict(x_train)\n",
    "        y_test_predict = tree_regressor.predict(x_test)\n",
    "\n",
    "        train_rmse.append(np.sqrt(mean_squared_error(y_train, y_train_predict)))\n",
    "        test_rmse.append(np.sqrt(mean_squared_error(y_test, y_test_predict)))\n",
    "\n",
    "    train_rmse_results.append(train_rmse)\n",
    "    test_rmse_results.append(test_rmse)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i, rs in enumerate(random_states):\n",
    "    plt.plot(tree_size, test_rmse_results[i], label=f\"Test RMSE (random_state={rs})\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Root Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.title('Effect of random_state on RMSE vs Tree Depth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get more acquainted with the results, check out what happens if you include less  `x_train` features. To do so, change what you need from the previous block of code and put it in the next block here.\n",
    "> Question: Do you see any important changes? How do you explain this?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de valores de max_features a probar\n",
    "max_features_values = [2, 4, 6, 10, 16]  \n",
    "\n",
    "# Rango de profundidades a explorar\n",
    "tree_size = np.arange(1, 20)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for max_feat in max_features_values:\n",
    "    train_rmse = []\n",
    "    test_rmse = []\n",
    "\n",
    "    for depth in tree_size:\n",
    "        tree_regressor = DecisionTreeRegressor(max_depth=depth, max_features=max_feat, random_state=1)\n",
    "        tree_regressor.fit(x_train, y_train)\n",
    "\n",
    "        y_train_predict = tree_regressor.predict(x_train)\n",
    "        y_test_predict = tree_regressor.predict(x_test)\n",
    "\n",
    "        train_rmse.append(np.sqrt(mean_squared_error(y_train, y_train_predict)))\n",
    "        test_rmse.append(np.sqrt(mean_squared_error(y_test, y_test_predict)))\n",
    "\n",
    "    # Graficar los resultados\n",
    "    plt.plot(tree_size, test_rmse, label=f\"Test RMSE (max_features={max_feat})\", linestyle=\"--\")\n",
    "    \n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Root Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.title('RMSE vs Tree Depth for Different max_features Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to complete this practical exercises with trees, we will also try out some classification trees to later do ensembles. Let us see how this works. \n",
    "\n",
    "First, load the `Carseat.csv` dataset from `data` (make sure to remove the NAs, as before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carseats = pd.read_csv('../data/Carseat.csv', sep = ',')\n",
    "carseats = carseats.dropna()\n",
    "carseats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make it so that we have a new binary variable called `high`. This variable should be `1` when `sales` are over 8, and `0` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "carseats['high'] = (carseats['Sales'] > 8).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the remaining variables to make them usable here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables `ShelveLoc`, `Urban` and `US` need to be converted to categorical variables to be correctly used. To that end, I suggest you use `pd.factorize` (although feel free to do as you will here...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "carseats['ShelveLoc'] = pd.factorize(carseats['ShelveLoc'])[0]\n",
    "carseats['Urban'] = pd.factorize(carseats['Urban'])[0]\n",
    "carseats['US'] = pd.factorize(carseats['US'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will employ all variables to predict the `high` value (except `Sales` and `high`, for obvious reasons). Note that we have essentially converted a _regression_ problem into a _binary classification_ one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X  =  carseats.drop(columns=['Sales', 'high'])\n",
    "y =  carseats['high']\n",
    "\n",
    "# Performn the train/test split with again 50% data for train and 50% for test \n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y,test_size=0.5, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a decision tree classifier. To control for the depth, we will fix it to a *maximum depth of 6*. Use as impurity criteria the **Gini index**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carseats_classifier = DecisionTreeClassifier(criterion='gini', max_depth=6, random_state=0)  # TODO: Fill the NAs. Fix the random_state to 0\n",
    "\n",
    "# Train the model with .fit\n",
    "carseats_classifier.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the tree (again, reuse whatever you may need here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "export_graphviz(carseats_classifier, \n",
    "                out_file=dot_data, \n",
    "                feature_names= X.columns, \n",
    "                filled=True, \n",
    "                class_names=None)\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now assess the quality of the tree. To that end, **represent the confusion matrix** for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = carseats_classifier.predict(X_test)\n",
    "matriz_confusion = confusion_matrix(y_test, predictions)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(matriz_confusion, annot=True, fmt='d', cmap='Blues', xticklabels=['Low', 'High'], yticklabels=['Low', 'High'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted_label')\n",
    "plt.ylabel('True_label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Questions: \n",
    "> * What is the **precision** of this tree?<br><br>\n",
    "0.746\n",
    "> * Do you consider the dataset balanced? <br><br> \n",
    "No esta muy desbalanceado, no obstante hay un mayor numero de trabajo de la clase Low que High"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembles\n",
    "\n",
    "Now we will try out some of the ensemble methods from class. Remember there is an stochastic component embedded in these for the most part, so we may not recover exactly the same results twice depending on how you implement things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree models mentioned above usually suffers from high variance. **B**ootstrap **agg**regation, or **bagging** usually helps with this issue. To do bagging here, we will do it both by hand and by employing the sklearn function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's go with the *by-hand* implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the bagging parameters\n",
    "n_estimators = 10  # Number of decision trees in the ensemble\n",
    "max_samples = 0.8  # Proportion of samples to be used for each bootstrap sample\n",
    "\n",
    "# Store the predictions\n",
    "predictions = []\n",
    "\n",
    "\n",
    "n_samples = X_train.shape[0]\n",
    "bootstrap_size = int(max_samples * n_samples)\n",
    "\n",
    "for _ in range(n_estimators):\n",
    "    \n",
    "    # Create a bootstrap sample\n",
    "    sample_indices = np.random.choice(n_samples, bootstrap_size, replace=True)\n",
    "    X_bootstrap = X_train.iloc[sample_indices]\n",
    "    y_bootstrap = y_train.iloc[sample_indices]\n",
    "    \n",
    "    # Train a decision tree classifier on the bootstrap sample\n",
    "    decision_tree = DecisionTreeClassifier()  # No restrictions\n",
    "    decision_tree.fit(X_bootstrap, y_bootstrap)\n",
    "    \n",
    "    # Make predictions on the test set using the trained decision tree\n",
    "    y_pred = decision_tree.predict(X_test)\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "# Convert predictions list to a NumPy array\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "# Combine predictions using majority voting (for classification)\n",
    "majority_vote = mode(predictions, axis=0).mode[0]\n",
    "\n",
    "# If this were regression, you would use averaging instead:\n",
    "combined_predictions = np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here! Use majority_vote and y_test\n",
    "clases = np.unique(y_test)\n",
    "num_clases = len(clases)\n",
    "matriz_confusion = np.zeros((num_clases, num_clases), dtype=int)\n",
    "\n",
    "#Con esto, obtenemos ínidices númericos respecto a cada etiqueta según las posibles clases exixtentes\n",
    "indices_reales = np.searchsorted(clases, y_test)\n",
    "indices_predichos = np.searchsorted(clases, majority_vote)\n",
    "\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    matriz_confusion[indices_reales[i], indices_predichos[i]] += 1\n",
    "\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(matriz_confusion)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(matriz_confusion, annot=True, fmt='d', cmap='Greens', xticklabels=['Low', 'High'], yticklabels=['Low', 'High'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted_label')\n",
    "plt.ylabel('True_label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this is done in `sklearn`... Fit it and show the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BaggingClassifier (fix random_state to 0)\n",
    "bagging = BaggingClassifier(random_state=0)\n",
    "\n",
    "#Train it with the training data\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "# Obtain the predictions\n",
    "bagging_pred = bagging.predict(X_test)\n",
    "\n",
    "#Print the confusion matrix (use the confusion_matrix function)\n",
    "mat_conf = confusion_matrix(y_test, bagging_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(mat_conf, annot=True, fmt='d', cmap='Oranges', xticklabels=['Low', 'High'], yticklabels=['Low', 'High'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted_label')\n",
    "plt.ylabel('True_label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `sklearn` implementation to study the variable importance. *Make sure you understand how this is done!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = np.mean(\n",
    "    [tree.feature_importances_ for tree in bagging.estimators_], axis=0)\n",
    "\n",
    "\n",
    "bagging_featureImportance = pd.DataFrame({'Feature Importance': feature_importances * 100}, index=X.columns)\n",
    "bagging_featureImportance.sort_values('Feature Importance', ascending=False).plot(kind='bar', color='red')\n",
    "plt.title('Feature Importance in Bagging')\n",
    "plt.ylabel('Importance (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Question: What do you see here? <br><br>Podemos observar que la variable más significativa es Price, con una influencia superior al 30%, lo que indica que es el factor que más impacta en las predicciones del modelo. Las demás variables también contribuyen a las decisiones del modelo, aunque su relevancia disminuye progresivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "We will also do this in the RF case. First, we will implement it by hand. Feel free to use the previous code and modify it as you may see fit to do RF here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# RF parameters\n",
    "n_estimators = 10  # Number of decision trees in the forest\n",
    "max_features = 0.8  # Proportion of features to consider for each split\n",
    "\n",
    "# Train decision trees with random feature selection and make predictions\n",
    "predictions = []\n",
    "for _ in range(n_estimators):\n",
    "\n",
    "    # TODO: Construct your own RF ensemble! Reuse the Bagging code and change whatever you may need here\n",
    "      # Create a bootstrap sample\n",
    "    sample_indices = np.random.choice(np.arange(0, X_train.shape[0], 1), size=int(X_train.shape[0]*max_samples), replace=True)# TODO\n",
    "    X_bootstrap = X_train.iloc[sample_indices]# TODO \n",
    "    y_bootstrap = y_train.iloc[sample_indices]# TODO \n",
    "    \n",
    "    \n",
    "    # Train a decision tree classifier on the bootstrap sample\n",
    "    decision_tree =   DecisionTreeClassifier(max_features=\"sqrt\")# TOD\n",
    "    decision_tree.fit(X_bootstrap, y_bootstrap)\n",
    "    \n",
    "    # Make predictions on the test set using the trained decision tree\n",
    "    y_pred = decision_tree.predict(X_test)# TODO: Obtain the predictions for X_test\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "\n",
    "# TODO: Finally, combine predictions using majority voting\n",
    "majority_vote = []# TODO\n",
    "for i in range(X_test.shape[0]):\n",
    "    true_count = 0\n",
    "    for t in predictions:\n",
    "        if t[i]:\n",
    "            true_count +=1\n",
    "    if true_count >= n_estimators/2:\n",
    "        majority_vote.append(True)\n",
    "    else:\n",
    "        majority_vote.append(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "confusion_matrix(y_test,majority_vote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us do it again with `sklearn` so you see the differences... Show the confusion matrix. In this last part there may be some differences with your run, which are due to the randomness of the classifiers constructed. Do not worry too much about it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the RF classifier with RandomForestClassifier. Fix the random_state to 0, n_estimators to 10 and max_features to 0.8\n",
    "rf = RandomForestClassifier(random_state=0,n_estimators=10,max_features=0.8)\n",
    "\n",
    "# Train it\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test values\n",
    "rf_pred= rf.predict(X_test)\n",
    "\n",
    "# Obtain the confusion matrix and print it\n",
    "confusion_matrix(y_test,rf_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using again the `sklearn` implementation, we will study the feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_featureImportance= pd.DataFrame({'Feature Importance':rf.feature_importances_*100}, index= X.columns)\n",
    "rf_featureImportance.sort_values('Feature Importance', ascending=False).plot(kind='bar', color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Question: Given the models thus far (simple tree, bagging and RF), which one would you choose and why? <br><br>El modelo que mejor performace tiene el de random forest y ya que podemos ver en la confusion matrix qeu los tru positives y true negatives , es decir los valores que estan bien clasificados son mayores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Boosting\n",
    "\n",
    "In order to fully complete our review of the ensemble methods from class, we are missing the **Boosting method**. In order to keep matters simple, we will implement it here with `sklearn` so that you get to see what it looks like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Create the Boosting model\n",
    "gb= GradientBoostingClassifier(n_estimators = 5000, random_state = 1, max_depth = 2)\n",
    "\n",
    "# If you want to try it out, you can change reuse most of previous codes to run it here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are pretty good! Keep in mind that this is achieved with super weak learners s.a. trees with depth 2. It is quite fast, and super easy to use with `sklearn`. We can also study the variable importance in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_featureimportance= pd.DataFrame({'Feature Importance': gb.feature_importances_*100}, index= X.columns)\n",
    "gb_featureimportance.sort_values('Feature Importance', ascending=False).plot(kind='bar', color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see how this can be implemented easily by hand, you can use the following code. We are using an implementation that follows a description of Boosting mode similar to the one given in the ISLR book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 1000  # Number of decision trees in the ensemble\n",
    "learning_rate = 0.1  # Learning rate for each decision tree\n",
    "\n",
    "# Initialize the weights for the training samples\n",
    "sample_weights = np.ones(len(X_train)) / len(X_train)\n",
    "\n",
    "# Train decision trees with weighted samples and make predictions\n",
    "predictions = []\n",
    "for _ in range(n_estimators):\n",
    "\n",
    "    # Train a decision tree classifier on the weighted training samples\n",
    "    decision_tree = DecisionTreeClassifier(max_depth = 2)\n",
    "    decision_tree.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "    # Make predictions on the test set using the trained decision tree\n",
    "    y_pred = decision_tree.predict(X_test)\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "    # Calculate error\n",
    "    incorrect = (y_pred != y_test).astype(int)\n",
    "    error = np.sum(sample_weights * incorrect) / np.sum(sample_weights)\n",
    "\n",
    "    # Update sample weights\n",
    "    alpha = learning_rate * np.log((1 - error) / error)\n",
    "    sample_weights *= np.exp(alpha * incorrect)\n",
    "\n",
    "# Combine predictions\n",
    "# For classification, you can use weighted voting\n",
    "combined_predictions = np.zeros(len(X_test))\n",
    "for prediction in predictions:\n",
    "    combined_predictions += prediction\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
